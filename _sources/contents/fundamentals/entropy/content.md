# Informal Explanation on Entropy

```{note}
- Entropy is an average of information or uncertainty of random variable.
- The number of bits tells us how uncertain an event is, as if we select one choice out of $2^\text{bit}$. 
```

The term `entropy` has been used across different fields such as physics, information theory, and cyber security. In information theory, I often find many articles using definitions as follows:

- Entropy refers degree of randomness or
- Entropy is the *expected* number of bits of information contained in each message, taken over all possibilities for the transmitted message.
- Entropy is an average level of "information", "surprise", or "uncertainty" of a variable's possible outcomes.

Having so various forms of explanation are somewhat tricky to understand, and it’s even more so if we mix them together. For this article, we pick one definition and run with it. And that is seeing entropy in terms of measurement for uncertainty.

$$
H(X) = E[I(X)] = -\sum log(P(x)) \cdot P(x)
$$

Essentially, entropy is an average (expected value) of information or uncertainty of a random variable. Let’s break this down and focus on the information I(x).

### Shannon Information I(x)

Informally, let’s say we want to have a function to measure uncertainty. We notice that generally high probability is the same as low uncertainty, and low probability is the same as high uncertainty. When you are 99% sure that it’s gonna rain, that’s super-low uncertainty.

$$
 I(x) = -log_2(x)
$$

Fortunately, these log operations give us just that. It’s able to convert a probability of 1.0 to 0 uncertainty and a probability of 0 to a high number of uncertainty. In other words, we can think of this information I(X) as a simple non-linear transformation function to measure uncertainty.

$$
\text{\#outcome} = 2^{\text{\#bit}} 
$$

What else can we think of this uncertainty? Suppose an image can be of 4 animals. For a case where an ML model says that an image has 50% being a cat. This is equivalent to log2(0.5) = **1 bit** of uncertainty. What this means is that although we have 4 possible animals, the chance of an image being a cat is the same as the chance of flipping a coin and getting a head. Notice that **one bit of uncertainty** is compared with **flipping one coin**. Alternatively, we roughly have 2^(1 bit)=2 possible choices to choose from.

```{figure} ./figures/cat_dog.png
---
name: cat-dog
width: 400px
align: center
---
Image generated by OpenAI’s Dall-E with prompt “painting of a dog that looks like a cat”
```

Similarly, suppose that image now has 1.25% of being a cat, that would be log2(0.125) = **3 bits** of uncertainty. This suggests that this image is as uncertain as tossing **a coin 3 three times** and getting all heads, or simply 2^(3 bits)=8 possible choices to pick from. So, we can say that this **bit unit** **tells us the odds of an event as if we are comparing the chance of a coin toss**.

### Entropy

Let’s move on to entropy. Entropy is an average of uncertainty across all events of a random variable. Informally, it tells how uncertain a random variable is. Is it as random tossing a coin, two coins, or n coins?

Let’s denote the animals earlier as random variable X that can have 4 possible outcomes such as cat, dog, mouse, and bird. Suppose we make a prediction on an image and the model says that the chance of being any animal is equally 0.25. In this case, the average of uncertainty (entropy) is 2 bits, meaning the odd is the same as tossing the coin two times and getting all heads. Or we have 2^2 choices to choose from.

You’d be like, duh! Obvious much. But let’s see the next example.

Instead, let’s say a cat has a chance of 90%, a dog has 4%, and a mouse and bird have 3% each. We would have the following.

$$
\begin{align*}
H(X) &= E[I(X)] \\
&= -\sum log(P(x)) \cdot P(x) \\
&= -\log_2(0.9)*0.9
-\log_2(0.04)*0.04
-2*\log_2(0.03)*0.03 \\
&=0.14+0.19+0.30 \\
&=0.63
\end{align*}
$$

The average uncertainty (entropy) of variable X is 0.63 bit, which is less than just tossing a coin (1 bit). Normally, we would have 4 possible outcomes (4 types of animals) to choose from. But if we apply the rule of this bit, we would have 2^0.63 = 1.54 outcomes. This implies that because we have a 90% chance of getting a cat, we may not deal with 4 choices anymore. It’s as if we now have 1.54 choices to choose from. And that’s what the entropy is for, which is to tell us how uncertain a random variable is. Is it as random as tossing 1 coin or 3 coins?

### Conclusion

To sum things up, there are many ways to explain entropy. In this article, we try to explain entropy that it’s an average uncertainty of a random variable. We know that the events themselves have a different chance of occurring. Entropy is just there to tell us how random it is as if we compare it to an odd of tossing some coins.

### Reference

- [https://brilliant.org/wiki/entropy-information-theory/](https://brilliant.org/wiki/entropy-information-theory/)
- [https://www.techtarget.com/whatis/definition/entropy](https://www.techtarget.com/whatis/definition/entropy)
- [https://en.wikipedia.org/wiki/Entropy_(information_theory)](https://en.wikipedia.org/wiki/Entropy_(information_theory))
- [https://en.wikipedia.org/wiki/Information_content](https://en.wikipedia.org/wiki/Information_content)
- [https://towardsdatascience.com/the-intuition-behind-shannons-entropy-e74820fe9800](https://towardsdatascience.com/the-intuition-behind-shannons-entropy-e74820fe9800)
- [https://machinelearningmastery.com/what-is-information-entropy/](https://machinelearningmastery.com/what-is-information-entropy/)
- [https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy](https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy)
- [https://www.youtube.com/watch?v=v68zYyaEmEA&t=330s&ab_channel=3Blue1Brown](https://www.youtube.com/watch?v=v68zYyaEmEA&t=330s&ab_channel=3Blue1Brown)